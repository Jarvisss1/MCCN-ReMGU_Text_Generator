{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jarvisss1/MCCN-ReMGU_Text_Generator/blob/main/MCCN_ReMGU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HieXmmHR158Z",
        "outputId": "125622af-0694-4199-f4eb-6672d3dc21ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download tokenizer resources\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Helper functions\n",
        "def read_words(filename):\n",
        "    \"\"\"Read words from a file, replacing newlines with <eos>.\"\"\"\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read().replace(\"\\n\", \" <eos> \").split()\n",
        "\n",
        "def build_vocab(filename):\n",
        "    \"\"\"Build a vocabulary from a dataset file.\"\"\"\n",
        "    data = read_words(filename)\n",
        "    counter = Counter(data)\n",
        "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        "    words, _ = zip(*count_pairs)\n",
        "    word_to_id = {word: i for i, word in enumerate(words)}\n",
        "    return word_to_id\n",
        "\n",
        "def file_to_word_ids(filename, word_to_id):\n",
        "    \"\"\"Convert words in a file to their corresponding IDs.\"\"\"\n",
        "    data = read_words(filename)\n",
        "    return [word_to_id.get(word, word_to_id.get(\"<unk>\", 1)) for word in data]\n",
        "\n",
        "def load_ptb_dataset(train_path, valid_path, test_path):\n",
        "    \"\"\"Load and preprocess the PTB dataset.\"\"\"\n",
        "    word_to_id = build_vocab(train_path)\n",
        "    train_data = file_to_word_ids(train_path, word_to_id)\n",
        "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
        "    test_data = file_to_word_ids(test_path, word_to_id)\n",
        "    vocab_size = len(word_to_id)\n",
        "    return train_data, valid_data, test_data, vocab_size, word_to_id\n",
        "\n",
        "\n",
        "class PTBDataset(Dataset):\n",
        "    def __init__(self, data, seq_length):\n",
        "        self.data = data\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.seq_length]\n",
        "        y = self.data[idx + self.seq_length]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/ptb.train.txt\"\n",
        "valid_path = \"/content/ptb.valid.txt\"\n",
        "test_path = \"/content/ptb.test.txt\"\n",
        "\n",
        "# Load dataset\n",
        "train_data, valid_data, test_data, vocab_size, word_to_id = load_ptb_dataset(train_path, valid_path, test_path)\n",
        "id_to_word = {id_: word for word, id_ in word_to_id.items()}\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Sample id_to_word: {dict(list(id_to_word.items())[:10])}\")  # Sample mapping\n",
        "\n",
        "# Parameters\n",
        "seq_length = 20\n",
        "batch_size = 32\n",
        "reduced_data_size = 300000\n",
        "\n",
        "# Reduce dataset size for faster experimentation\n",
        "train_data = train_data[:reduced_data_size]\n",
        "\n",
        "# DataLoader for training\n",
        "train_dataset = PTBDataset(train_data, seq_length)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\n",
        "\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MGUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MGUCell, self).__init__()\n",
        "        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.W_a = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        combined = torch.cat([x, h_prev], dim=1)\n",
        "        f_t = torch.sigmoid(self.W_f(combined))\n",
        "        a_t = torch.tanh(self.W_a(combined))\n",
        "        h_t = f_t * h_prev + (1 - f_t) * a_t\n",
        "        return h_t\n",
        "\n",
        "class MGU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(MGU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.cells = nn.ModuleList([MGUCell(input_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, h_0):\n",
        "        seq_len, batch_size, _ = x.size()\n",
        "        h = h_0\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            for i, cell in enumerate(self.cells):\n",
        "                h[i] = cell(x[t], h[i])\n",
        "            outputs.append(h[-1])\n",
        "        outputs = torch.stack(outputs, dim=0)\n",
        "        return outputs, h\n",
        "\n",
        "class ResidualMGUNetwork(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, cnn_channels, hidden_size, num_layers, output_size):\n",
        "        super(ResidualMGUNetwork, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # Multi-window convolution: kernel sizes 3, 4, 5\n",
        "        self.kernel_sizes = [3, 5,7]\n",
        "        assert cnn_channels % len(self.kernel_sizes) == 0, \"cnn_channels must be divisible by number of kernels\"\n",
        "        self.per_kernel_channels = cnn_channels // len(self.kernel_sizes)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(\n",
        "                in_channels=embed_size,\n",
        "                out_channels=self.per_kernel_channels,\n",
        "                kernel_size=k,\n",
        "                padding=k // 2  # Same-length output\n",
        "            )\n",
        "            for k in self.kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.mgu = MGU(cnn_channels, hidden_size, num_layers)\n",
        "        self.residual_fc = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_length)\n",
        "        embedded = self.embedding(x)  # (batch, seq_len, embed_size)\n",
        "        embedded = embedded.permute(0, 2, 1)  # (batch, embed_size, seq_len)\n",
        "\n",
        "        # Apply each convolution and ReLU\n",
        "        conv_outputs = [F.relu(conv(embedded)) for conv in self.convs]  # List of (batch, C, seq_len)\n",
        "        cnn_out = torch.cat(conv_outputs, dim=1)  # (batch, cnn_channels, seq_len)\n",
        "\n",
        "        # Prepare for RNN: (seq_len, batch, cnn_channels)\n",
        "        cnn_out = cnn_out.permute(2, 0, 1)\n",
        "\n",
        "        batch_size = cnn_out.size(1)\n",
        "        h_0 = [torch.zeros(batch_size, self.mgu.hidden_size, device=x.device) for _ in range(self.mgu.num_layers)]\n",
        "\n",
        "        mgu_out, _ = self.mgu(cnn_out, h_0)  # (seq_len, batch, hidden_size)\n",
        "        last_mgu_out = mgu_out[-1]  # (batch, hidden_size)\n",
        "\n",
        "        # Residual connection\n",
        "        residual_out = F.relu(self.residual_fc(last_mgu_out)) + last_mgu_out\n",
        "        residual_out = self.dropout(residual_out)\n",
        "        output = self.fc(residual_out)\n",
        "        return F.log_softmax(output, dim=1)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "vocab_size = 10000\n",
        "embed_size = 128\n",
        "cnn_channels = 150  # Example value\n",
        "assert cnn_channels % 3 == 0, \"Make sure this is divisible by 3\"\n",
        "\n",
        "hidden_size = 150\n",
        "num_layers = 1\n",
        "output_size = vocab_size\n",
        "\n",
        "model = ResidualMGUNetwork(vocab_size, embed_size, cnn_channels, hidden_size, num_layers, output_size)\n",
        "sample_input = torch.randint(0, vocab_size, (20, 35))\n",
        "output = model(sample_input)\n",
        "print(output.shape)  # Should be (Batch, Output Size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGOQrfpI2I0r",
        "outputId": "72f74e79-458b-4eae-b75b-124d21111f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10000\n",
            "Sample id_to_word: {0: 'the', 1: '<unk>', 2: '<eos>', 3: 'N', 4: 'of', 5: 'to', 6: 'a', 7: 'in', 8: 'and', 9: \"'s\"}\n",
            "Number of training batches: 9374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Initialize the model\n",
        "vocab_size = 10000\n",
        "embed_size = 128\n",
        "cnn_channels = 150\n",
        "hidden_size = 150\n",
        "num_layers = 1\n",
        "output_size = vocab_size\n",
        "\n",
        "model = ResidualMGUNetwork(vocab_size, embed_size, cnn_channels, hidden_size, num_layers, output_size)\n",
        "\n",
        "# Training Parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)  # Increase LR by 50% each epoch\n",
        "\n",
        "num_epochs = 15\n",
        "\n",
        "def calculate_perplexity(loss):\n",
        "    return torch.exp(torch.tensor(loss))\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} - Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "    epoch_iterator = tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\")\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(epoch_iterator):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient Clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "        avg_loss = total_loss / (batch_idx + 1)\n",
        "        perplexity = calculate_perplexity(avg_loss)\n",
        "        speed = (batch_idx + 1) * batch_size / (time.time() - start_time)\n",
        "\n",
        "        epoch_iterator.set_postfix({\n",
        "            \"Batch Loss\": f\"{loss.item():.4f}\",\n",
        "            \"Perplexity\": f\"{perplexity:.2f}\",\n",
        "            \"Speed (words/s)\": f\"{speed:.0f}\",\n",
        "            \"Accuracy\": f\"{100 * correct / total:.2f}%\",\n",
        "            \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"\n",
        "        })\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    perplexity = calculate_perplexity(avg_loss)\n",
        "    print(f\"Epoch {epoch + 1} Completed: Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}, Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv_8AgW-2MDe",
        "outputId": "8acde05c-496c-4669-8985-5e75b2977169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 - Learning Rate: 0.001000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 9374/9374 [03:20<00:00, 46.83it/s, Batch Loss=6.2617, Perplexity=504.57, Speed (words/s)=1499, Accuracy=13.18%, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Completed: Loss: 6.2237, Perplexity: 504.57, Accuracy: 13.18%\n",
            "Epoch 2/15 - Learning Rate: 0.000900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 9374/9374 [03:15<00:00, 48.03it/s, Batch Loss=5.7313, Perplexity=321.79, Speed (words/s)=1537, Accuracy=16.16%, LR=0.000900]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Completed: Loss: 5.7739, Perplexity: 321.79, Accuracy: 16.16%\n",
            "Epoch 3/15 - Learning Rate: 0.000810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 9374/9374 [03:25<00:00, 45.72it/s, Batch Loss=6.1427, Perplexity=266.47, Speed (words/s)=1463, Accuracy=17.59%, LR=0.000810]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Completed: Loss: 5.5852, Perplexity: 266.47, Accuracy: 17.59%\n",
            "Epoch 4/15 - Learning Rate: 0.000729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 9374/9374 [03:24<00:00, 45.86it/s, Batch Loss=5.0688, Perplexity=233.35, Speed (words/s)=1468, Accuracy=18.58%, LR=0.000729]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Completed: Loss: 5.4526, Perplexity: 233.35, Accuracy: 18.58%\n",
            "Epoch 5/15 - Learning Rate: 0.000656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 9374/9374 [03:21<00:00, 46.47it/s, Batch Loss=5.9631, Perplexity=209.36, Speed (words/s)=1487, Accuracy=19.55%, LR=0.000656]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Completed: Loss: 5.3440, Perplexity: 209.36, Accuracy: 19.55%\n",
            "Epoch 6/15 - Learning Rate: 0.000590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 9374/9374 [03:20<00:00, 46.71it/s, Batch Loss=6.2092, Perplexity=190.74, Speed (words/s)=1495, Accuracy=20.35%, LR=0.000590]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Completed: Loss: 5.2509, Perplexity: 190.74, Accuracy: 20.35%\n",
            "Epoch 7/15 - Learning Rate: 0.000531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 9374/9374 [03:23<00:00, 46.15it/s, Batch Loss=5.9597, Perplexity=175.30, Speed (words/s)=1477, Accuracy=21.04%, LR=0.000531]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Completed: Loss: 5.1665, Perplexity: 175.30, Accuracy: 21.04%\n",
            "Epoch 8/15 - Learning Rate: 0.000478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 9374/9374 [03:24<00:00, 45.78it/s, Batch Loss=4.8221, Perplexity=162.50, Speed (words/s)=1465, Accuracy=21.79%, LR=0.000478]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Completed: Loss: 5.0907, Perplexity: 162.50, Accuracy: 21.79%\n",
            "Epoch 9/15 - Learning Rate: 0.000430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 9374/9374 [03:24<00:00, 45.93it/s, Batch Loss=6.3412, Perplexity=150.96, Speed (words/s)=1470, Accuracy=22.38%, LR=0.000430]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Completed: Loss: 5.0170, Perplexity: 150.96, Accuracy: 22.38%\n",
            "Epoch 10/15 - Learning Rate: 0.000387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 9374/9374 [03:19<00:00, 47.05it/s, Batch Loss=5.0232, Perplexity=141.52, Speed (words/s)=1506, Accuracy=22.95%, LR=0.000387]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Completed: Loss: 4.9525, Perplexity: 141.52, Accuracy: 22.95%\n",
            "Epoch 11/15 - Learning Rate: 0.000349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 9374/9374 [03:21<00:00, 46.43it/s, Batch Loss=5.2873, Perplexity=133.09, Speed (words/s)=1486, Accuracy=23.45%, LR=0.000349]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Completed: Loss: 4.8910, Perplexity: 133.09, Accuracy: 23.45%\n",
            "Epoch 12/15 - Learning Rate: 0.000314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 9374/9374 [03:17<00:00, 47.50it/s, Batch Loss=4.7629, Perplexity=126.00, Speed (words/s)=1520, Accuracy=24.02%, LR=0.000314]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Completed: Loss: 4.8363, Perplexity: 126.00, Accuracy: 24.02%\n",
            "Epoch 13/15 - Learning Rate: 0.000282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 9374/9374 [03:18<00:00, 47.16it/s, Batch Loss=5.9654, Perplexity=119.13, Speed (words/s)=1509, Accuracy=24.51%, LR=0.000282]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Completed: Loss: 4.7802, Perplexity: 119.13, Accuracy: 24.51%\n",
            "Epoch 14/15 - Learning Rate: 0.000254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14: 100%|██████████| 9374/9374 [03:22<00:00, 46.33it/s, Batch Loss=4.6739, Perplexity=113.45, Speed (words/s)=1483, Accuracy=24.95%, LR=0.000254]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Completed: Loss: 4.7313, Perplexity: 113.45, Accuracy: 24.95%\n",
            "Epoch 15/15 - Learning Rate: 0.000229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15: 100%|██████████| 9374/9374 [03:23<00:00, 45.99it/s, Batch Loss=5.3813, Perplexity=108.51, Speed (words/s)=1472, Accuracy=25.31%, LR=0.000229]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Completed: Loss: 4.6869, Perplexity: 108.51, Accuracy: 25.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Save the model\n",
        "model_path = \"residual_mgu_model.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsWUtP2x2_fx",
        "outputId": "574d53ee-7bdb-4146-ddf8-0725cbd51968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to residual_mgu_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming valid_data and test_data are already loaded and preprocessed\n",
        "\n",
        "# Parameters\n",
        "seq_length = 20\n",
        "batch_size = 32\n",
        "\n",
        "# Create DataLoaders\n",
        "valid_dataset = PTBDataset(valid_data, seq_length)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset = PTBDataset(test_data, seq_length)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Evaluate on Validation Set\n",
        "val_loss, val_accuracy = evaluate(model, valid_loader, criterion)\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8R3TXVGnfjy",
        "outputId": "00e8125f-639b-4a22-cab7-ac3009b5c7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 5.7652, Validation Accuracy: 19.75%\n",
            "Test Loss: 5.6367, Test Accuracy: 19.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def apply_temperature(logits, temperature=1.0):\n",
        "    logits = logits / temperature\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "    return probabilities\n",
        "\n",
        "def generate_text(model, start_sequence, word_to_id, id_to_word, num_words=10, temperature=1.0):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device  # get model's device\n",
        "    generated_sequence = start_sequence.copy()\n",
        "\n",
        "    # Convert start_sequence to IDs\n",
        "    input_ids = [word_to_id.get(word, word_to_id.get(\"<unk>\", 0)) for word in start_sequence]\n",
        "\n",
        "    largest_kernel_size = 5\n",
        "    if len(input_ids) < largest_kernel_size:\n",
        "        padding = [0] * (largest_kernel_size - len(input_ids))\n",
        "        input_ids = padding + input_ids\n",
        "\n",
        "    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)  # Send to correct device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_words):\n",
        "            logits = model(input_tensor)  # [1, vocab_size]\n",
        "            probabilities = apply_temperature(logits[0], temperature)\n",
        "\n",
        "            next_word_id = torch.multinomial(probabilities, 1).item()\n",
        "            next_word = id_to_word.get(next_word_id, \"<unk>\")\n",
        "            generated_sequence.append(next_word)\n",
        "\n",
        "            input_ids.append(next_word_id)\n",
        "            input_ids = input_ids[-largest_kernel_size:]\n",
        "            input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)  # Ensure consistent device\n",
        "\n",
        "    return generated_sequence\n"
      ],
      "metadata": {
        "id": "MJkwe-uwnrUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_sequence = [\"investors\", \"always\", \"dont\"]\n",
        "generated_sequence = generate_text(model, start_sequence, word_to_id, id_to_word, num_words=20, temperature=0.8)\n",
        "print(\"Generated Sequence:\", \" \".join(generated_sequence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fceqkmw3oRYE",
        "outputId": "96a11f90-8cb5-4fcd-965a-1f9690ef46c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: investors always dont the <unk> of him <eos> the adviser have received the company <eos> it is just of the u.s. aircraft all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_sequence = [\"washington\", \"is\", \"in\"]\n",
        "generated_sequence = generate_text(model, start_sequence, word_to_id, id_to_word, num_words=20, temperature=0.8)\n",
        "print(\"Generated Sequence:\", \" \".join(generated_sequence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssSC1UoLolJJ",
        "outputId": "8fae8e24-c954-4286-80c8-90e5c6c5e13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: washington is in ruling <eos> the mexico is <unk> an <unk> <unk> cut <eos> the department paul <unk> <unk> <eos> they <unk> <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(probabilities, k=10):\n",
        "    top_probs, top_indices = torch.topk(probabilities, k)\n",
        "    top_probs = top_probs / top_probs.sum()  # Normalize\n",
        "    next_word_id = torch.multinomial(top_probs, 1).item()\n",
        "    return top_indices[next_word_id].item()\n"
      ],
      "metadata": {
        "id": "TKsOz6BhpEi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def apply_temperature(logits, temperature=1.0):\n",
        "    logits = logits / temperature\n",
        "    return F.softmax(logits, dim=-1)\n",
        "\n",
        "def top_k_sampling(probabilities, k=10):\n",
        "    top_probs, top_indices = torch.topk(probabilities, k)\n",
        "    top_probs = top_probs / top_probs.sum()  # Normalize\n",
        "    next_word_id = torch.multinomial(top_probs, 1).item()\n",
        "    return top_indices[next_word_id].item()\n",
        "\n",
        "def generate_text(model, start_sequence, word_to_id, id_to_word, num_words=10, temperature=1.0, top_k=None):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device  # get model's device\n",
        "    generated_sequence = start_sequence.copy()\n",
        "\n",
        "    # Convert start words to IDs\n",
        "    input_ids = [word_to_id.get(word, word_to_id.get(\"<unk>\", 0)) for word in start_sequence]\n",
        "\n",
        "    # Ensure minimum input length to match largest CNN kernel\n",
        "    largest_kernel_size = 5\n",
        "    if len(input_ids) < largest_kernel_size:\n",
        "        input_ids = [0] * (largest_kernel_size - len(input_ids)) + input_ids\n",
        "\n",
        "    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)  # (1, seq_len)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_words):\n",
        "            logits = model(input_tensor)  # (1, vocab_size)\n",
        "            probs = apply_temperature(logits[0], temperature)  # (vocab_size)\n",
        "\n",
        "            if top_k:\n",
        "                next_word_id = top_k_sampling(probs, top_k)\n",
        "            else:\n",
        "                next_word_id = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            next_word = id_to_word.get(next_word_id, \"<unk>\")\n",
        "            generated_sequence.append(next_word)\n",
        "\n",
        "            # Slide the window\n",
        "            input_ids.append(next_word_id)\n",
        "            input_ids = input_ids[-largest_kernel_size:]\n",
        "            input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "\n",
        "    return generated_sequence\n"
      ],
      "metadata": {
        "id": "ope3eH4ppIYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_sequence = [\"washington\", \"is\", \"in\"]\n",
        "# start_sequence = [\"the\", \"film\", \"was\"]\n",
        "generated = generate_text(\n",
        "    model,\n",
        "    start_sequence,\n",
        "    word_to_id,\n",
        "    id_to_word,\n",
        "    num_words=20,\n",
        "    temperature=0.8,   # Lower = more confident, higher = more random\n",
        "    top_k=10           # Optional: sample only from top 10 likely words\n",
        ")\n",
        "\n",
        "print(\"Generated:\", \" \".join(generated))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbajVWovpLdQ",
        "outputId": "6d6ad791-ba80-4abb-c23a-abd74275c3de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: washington is in the country as the <unk> of the company 's the <unk> of the <unk> and <unk> <unk> and <unk> <unk>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_sequence = [\"stock\", \"trading\", \"in\"]\n",
        "generated_sequence = generate_text(\n",
        "    model,\n",
        "    start_sequence,\n",
        "    word_to_id,\n",
        "    id_to_word,\n",
        "    num_words=15,\n",
        "    temperature=0.8,\n",
        "    top_k=10  # Only sample from top 10 words\n",
        ")\n",
        "\n",
        "print(\"Generated:\", \" \".join(generated_sequence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNR54Dy4pRvn",
        "outputId": "d76a07dd-21a1-4228-c8af-ffbfade709df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: stock trading in recent weeks <eos> the company has been sold <eos> in the u.s. and other <unk>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_sequence = [\"stock\", \"trading\", \"in\"]\n",
        "generated_sequence = generate_text(\n",
        "    model,\n",
        "    start_sequence,\n",
        "    word_to_id,\n",
        "    id_to_word,\n",
        "    num_words=20,\n",
        "    temperature=0.8,\n",
        "    top_k=10  # Only sample from top 10 words\n",
        ")\n",
        "\n",
        "print(\"Generated:\", \" \".join(generated_sequence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRdMegIJpciQ",
        "outputId": "a0c030cb-7447-4455-8dd8-28e1b9ab2f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: stock trading in N the new york court in the u.s. <eos> it said it is <unk> the <unk> <unk> and <unk> and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    perplexity = math.exp(avg_loss)  # <- Perplexity added here\n",
        "    return avg_loss, accuracy, perplexity\n"
      ],
      "metadata": {
        "id": "-JZ9Y-yOqGr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_accuracy, val_ppl = evaluate(model, valid_loader, criterion)\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%, Perplexity: {val_ppl:.2f}\")\n",
        "\n",
        "test_loss, test_accuracy, test_ppl = evaluate(model, test_loader, criterion)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%, Perplexity: {test_ppl:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO_MUqXLqJGC",
        "outputId": "53e6a81b-2fa1-43bb-c854-0cb63c9dcb21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 5.7652, Accuracy: 19.75%, Perplexity: 319.00\n",
            "Test Loss: 5.6367, Accuracy: 19.53%, Perplexity: 280.54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "embed_size = 128\n",
        "cnn_channels = 150\n",
        "hidden_size = 150\n",
        "num_layers = 1\n",
        "output_size = vocab_size\n",
        "\n",
        "# Instantiate the model\n",
        "model = ResidualMGUNetwork(vocab_size, embed_size, cnn_channels, hidden_size, num_layers, output_size)\n",
        "\n",
        "# Move to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Load the saved weights\n",
        "model_path = \"residual_mgu_model.pth\"\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "print(\"✅ Model loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6w_76dqLVSF",
        "outputId": "7d902963-53fd-4452-919e-98361854cfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_sequence = [\"stock\", \"trading\", \"in\"]\n",
        "generated_sequence = generate_text(\n",
        "    model,\n",
        "    start_sequence,\n",
        "    word_to_id,\n",
        "    id_to_word,\n",
        "    num_words=20,\n",
        "    temperature=0.8,\n",
        "    top_k=10  # Optional\n",
        ")\n",
        "\n",
        "print(\"Generated Sequence:\", \" \".join(generated_sequence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qAYGJHJLuWP",
        "outputId": "665c0013-82d7-4b5f-bb92-6d690a80f795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: stock trading in the u.s. <eos> the <unk> <unk> is a <unk> to be <unk> on the <unk> business <eos> it 's <unk>\n"
          ]
        }
      ]
    }
  ]
}